{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc7e3f3",
   "metadata": {},
   "source": [
    "### A prompt is a statement or instruction given to elicit a specific response or action, often used in conversations, writing exercises, programming interfaces, or AI interactions like ours. In the context of AI, a prompt is the input provided to the model to generate a desired output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76693eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: life is a stage\n",
      "Generated Text: life is a stage of development that is not easily understood.\n",
      "\n",
      "The stage is the stage where the brain is able to process information. The brain's ability to understand information is called the \"brain's \"memory\".\n",
      ". This is where information\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Encode the prompt into the input IDs\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text using the model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"life is a stage\"\n",
    "generated_text = generate_text(prompt, max_length=50)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06bf3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt: hi all\n",
      "Do you want the output in bullet points? (yes/no): yes\n",
      "Prompt:\n",
      "- hi all\n",
      "Generated Text:\n",
      "- hi all over the place.\n",
      "- \n",
      "- \"I'm not sure if I'm going to be able to do it,\" he said. \"I don't know if it's going be a good thing or not. I don' know. It's\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Encode the prompt into the input IDs\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text using the model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Function to format text as bullet points\n",
    "def format_as_bullet_points(text):\n",
    "    return \"\\n\".join(f\"- {line}\" for line in text.split(\"\\n\"))\n",
    "\n",
    "# Interactive prompt input\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "# Ask user if they want the output in bullet points\n",
    "output_bullet_points = input(\"Do you want the output in bullet points? (yes/no): \").lower().strip() == \"yes\"\n",
    "\n",
    "# Generate text based on the input prompt\n",
    "generated_text = generate_text(prompt, max_length=50)\n",
    "\n",
    "# Format the generated text as bullet points if requested\n",
    "if output_bullet_points:\n",
    "    generated_text = format_as_bullet_points(generated_text)\n",
    "\n",
    "# Print the prompt and the generated text\n",
    "print(\"Prompt:\")\n",
    "print(f\"- {prompt}\")\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
