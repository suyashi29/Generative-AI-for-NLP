{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ca2da3",
   "metadata": {},
   "source": [
    "### The Hugging Face transformers library provides a variety of pretrained models and pipelines that can perform different natural language processing (NLP) tasks beyond sentiment analysis. Below are some examples of tasks you can perform using different pipelines provided by the transformers library, along with a set of functions demonstrating these capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bb5d7",
   "metadata": {},
   "source": [
    "pip install wordcloud --trusted-host pypi.org --trusted-host files.pythonhosted.org transformers==4.9.2 torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891a4217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998834133148193}]\n"
     ]
    }
   ],
   "source": [
    "#1. Text Classification (other than sentiment analysis)\n",
    "#Function: Classify text into predefined categories.\n",
    "from transformers import pipeline\n",
    "# Explicitly specify the model name\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "def classify_text(text, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "    classifier = pipeline(\"text-classification\", model=model_name)\n",
    "    results = classifier(text)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a fantastic product!\"\n",
    "print(classify_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5cf549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9957955, 'word': 'Ashi', 'start': 0, 'end': 4}, {'entity_group': 'LOC', 'score': 0.9983322, 'word': 'Delhi', 'start': 16, 'end': 21}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def recognize_entities(text):\n",
    "    model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "    ner_pipeline = pipeline(\"ner\", model=model_name, grouped_entities=True)\n",
    "    results = ner_pipeline(text)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text = \"Ashi is born in Delhi.\"\n",
    "print(recognize_entities(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25bdd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a211382fb5428c90f11dab9bf7437e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyashi144893\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\suyashi144893\\.cache\\huggingface\\hub\\models--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6672ac0436724825b92a35e9e21ca4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1330b8bfe94de2aa58348667469358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ebc1dbf09b427b91754421adc6bdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b726b9dd73814f28b3d4cee5cfea013b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9752461910247803, 'start': 40, 'end': 53, 'answer': 'New York City'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def answer_question(question, context):\n",
    "    model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "    results = qa_pipeline(question=question, context=context)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "context = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "question = \"Where is Hugging Face Inc. based?\"\n",
    "print(answer_question(question, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c605f305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good human embodies kindness, empathy, and integrity. They act selflessly, helping others and showing compassion. Honesty and respect guide their interactions, fostering trust and positive relationships.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize_text(text, max_length=50, min_length=25):\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    summarization_pipeline = pipeline(\"summarization\", model=model_name)\n",
    "    summary = summarization_pipeline(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Example usage\n",
    "text = \"A good human embodies kindness, empathy, and integrity. They act selflessly, helping others and showing compassion. Honesty and respect guide their interactions, fostering trust and positive relationships. A good human values diversity, promotes equality, and strives to make the world a better place through their actions and understanding.\"\n",
    "print(summarize_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c8103f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A White Rabbit! The Jigsaw Witch The Journey ULTIMATE MATCH FINAL EDITION THE KING OF FIGHTERS 2002 UNLIMITED MATCH THE KING OF FIGHTERS XIII STEAM EDITION THE KING OF FIGHTERS XIV STEAM EDITION The King\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    model_name = \"gpt2\"\n",
    "    text_generator = pipeline(\"text-generation\", model=model_name)\n",
    "    generated_text = text_generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    return generated_text[0]['generated_text']\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A White Rabbit\"\n",
    "print(generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f79f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un joli chat\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def translate_text(text, source_lang=\"en\", target_lang=\"fr\"):\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n",
    "    translation_pipeline = pipeline(\"translation\", model=model_name)\n",
    "    translation = translation_pipeline(text)\n",
    "    return translation[0]['translation_text']\n",
    "\n",
    "# Example usage\n",
    "text = \"A pretty cat\"\n",
    "print(translate_text(text, source_lang=\"en\", target_lang=\"fr\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
