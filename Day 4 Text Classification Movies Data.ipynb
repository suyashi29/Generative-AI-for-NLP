{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d338f646",
   "metadata": {},
   "source": [
    "### Text Classification is the process of assigning a label or category to a given piece of text. For example, we can classify emails as spam or not spam, tweets as positive or negative, and articles as relevant or not relevant to a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22a54f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Movie                                Review  Sentiment\n",
      "0     Pulp Fiction  The special effects were incredible.          0\n",
      "1  The Dark Knight          The film lacked originality.          1\n",
      "2     Forrest Gump    The screenplay was poorly written.          1\n",
      "3       Fight Club                     Predictable plot.          0\n",
      "4  The Dark Knight                 Disappointing ending.          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/suyashi29/Generative-AI-for-NLP/main/movie_reviews.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae4aad",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Preprocessing the text data to clean and prepare it for modeling.\n",
    "\n",
    "- Lowercasing: Convert all text to lowercase.\n",
    "- Removing Punctuation: Remove punctuation marks.\n",
    "- Tokenization: Split the text into words.\n",
    "- Removing Stopwords: Remove common words that do not carry significant meaning.\n",
    "- Stemming/Lemmatization: Reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebdd29",
   "metadata": {},
   "source": [
    "was: be\n",
    "sings:sing\n",
    "worst: bad\n",
    "sings:sing\n",
    "danced:dance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdabbbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suyashi144893\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyashi144893\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suyashi144893\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Movie                     Review  Sentiment\n",
      "0     Pulp Fiction  special effect incredible          0\n",
      "1  The Dark Knight    film lacked originality          1\n",
      "2     Forrest Gump  screenplay poorly written          1\n",
      "3       Fight Club           predictable plot          0\n",
      "4  The Dark Knight       disappointing ending          1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()  \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the Review column\n",
    "data['Review'] = data['Review'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ca432",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d1ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200,) (800,) (3200,) (800,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and labels\n",
    "X = data['Review']\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the splits\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf10ea7",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Convert the text data into numerical data using TF-IDF Vectorizer.\n",
    "\n",
    "## The difference between vectorizer.fit_transform(X_train) and vectorizer.transform(X_test) lies in the fitting and transforming processes performed on training and testing datasets.\n",
    "\n",
    "#### vectorizer.fit_transform(X_train)\n",
    "- fit: The vectorizer.fit(X_train) step analyzes the training data X_train to learn the vocabulary and the idf (inverse document frequency) statistics (if applicable).\n",
    "- transform: The vectorizer.transform(X_train) step converts the training data into a matrix of token counts or tf-idf features based on the learned vocabulary.\n",
    "- Combining these into vectorizer.fit_transform(X_train) allows you to both fit the model and transform the training data in a single step. This step produces a sparse matrix or another representation suitable for machine learning algorithms.\n",
    "\n",
    "#### vectorizer.transform(X_test)\n",
    "- transform: The vectorizer.transform(X_test) step converts the test data X_test into the same feature space as the training data. It uses the vocabulary and idf statistics learned from X_train during the fit step.\n",
    "- This ensures that the test data is represented in the same way as the training data, allowing for accurate evaluation of the model trained on X_train.\n",
    "\n",
    "### Summary\n",
    "vectorizer.fit_transform(X_train): Fits the vectorizer to the training data and transforms the training data into the desired feature representation.\n",
    "vectorizer.transform(X_test): Transforms the test data into the same feature representation using the vocabulary and statistics learned from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4c6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 54) (800, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Display the shape of the transformed data\n",
    "print(X_train_tfidf.shape, X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f29ee",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Model Training\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model. Hyperparameters are the parameters that are set before the learning process begins and cannot be learned directly from the training data. Examples include the learning rate in neural networks, the number of trees in a random forest, and the regularization parameter in logistic regression.\n",
    "\n",
    "### Grid Search\n",
    "- is one of the most commonly used methods for hyperparameter tuning. It involves an exhaustive search through a manually specified subset of the hyperparameter space. The steps involved in the Grid Search method are as follows:\n",
    "\n",
    "- Define the Hyperparameter Space: Specify the hyperparameters to be tuned and their possible values. This is often done by creating a grid of parameter values.\n",
    "\n",
    "- Perform Cross-Validation: For each combination of hyperparameters, the model is trained and evaluated using cross-validation. Cross-validation helps in assessing the model's performance on different subsets of the training data.\n",
    "\n",
    "- Evaluate Performance: The performance metric (e.g., accuracy, F1 score, etc.) is calculated for each combination of hyperparameters.\n",
    "\n",
    "- Select the Best Hyperparameters: The combination of hyperparameters that yields the best performance on the validation set is selected as the optimal set.\n",
    "\n",
    "\n",
    "\n",
    "### here we are using GridSearchCV to find the best hyperparameters for the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f13b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters: {'C': 0.1, 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c9bab",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Train a classification model, for example, a Logistic Regression model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "45975721",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Display the training accuracy\n",
    "print(f\"Training Accuracy: {model.score(X_train_tfidf, y_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6310f7",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d8765c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.49875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       0.51      0.43      0.47       408\n",
      "     Class 2       0.49      0.57      0.53       392\n",
      "\n",
      "    accuracy                           0.50       800\n",
      "   macro avg       0.50      0.50      0.50       800\n",
      "weighted avg       0.50      0.50      0.50       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Display classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2']))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c2d65fa",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Display classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37107bb",
   "metadata": {},
   "source": [
    "## Save the Model and Vectorizer\n",
    "Save the trained model and the vectorizer for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d5c102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'logistic_regression_best_model.pkl')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7996809",
   "metadata": {},
   "source": [
    "## Load and Use the Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5356bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Load the model and vectorizer\n",
    "best_model = joblib.load('logistic_regression_best_model.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Sample new reviews\n",
    "new_reviews = [\"The movie was fantastic and excellent!\", \"I did not like the film, it was boring.\",\"i am happy with movie\"]\n",
    "\n",
    "# Preprocess and vectorize the new reviews\n",
    "new_reviews_preprocessed = [preprocess_text(review) for review in new_reviews]\n",
    "new_reviews_tfidf = vectorizer.transform(new_reviews_preprocessed)\n",
    "\n",
    "# Predict sentiments\n",
    "predictions = best_model.predict(new_reviews_tfidf)\n",
    "\n",
    "# Map predictions to labels\n",
    "label_mapping = {0: 'positive', 1: 'negative'}\n",
    "labeled_predictions = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "print(labeled_predictions)  # Output: ['positive', 'negative']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "464d2ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
